{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Use project modules where convenient\n",
    "from MachineLearningModule.Classifiers import Classifiers\n",
    "from MachineLearningModule.DataPreProcessing import Conversions, HandleMissingValues, Normalisation, DimReduction\n",
    "\n",
    "DATA_PATH = 'Datasets/Drebin_v1_sample.csv' if os.path.exists('Datasets/Drebin_v1_sample.csv') else 'Datasets/Drebin_v1.csv'\n",
    "print('Loading', DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "target = 'class'\n",
    "if target not in df.columns:\n",
    "    raise ValueError(f'Expected target column \n",
    " in dataset')\n",
    "\n",
    "# Preprocessing pipeline (reuse functions from project)\n",
    "df = Conversions.convert_all_cat_features_to_num_via_label_encoding(df)\n",
    "df = HandleMissingValues.impute_missing_values_with_feature_mean(df)\n",
    "df = Normalisation.min_max_normalisation(df)\n",
    "\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Quick train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "models = ['RF', 'SVM', 'KNN', 'NB', 'MLP']\n",
    "results = {}\n",
    "\n",
    "for name in models:\n",
    "    print('Training', name)\n",
    "    try:\n",
    "        clf = Classifiers.get_classifier(name, X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print('Error training', name, e)\n",
    "        continue\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='binary', pos_label=1) if len(np.unique(y))==2 else precision_score(y_test, y_pred, average='macro')\n",
    "    rec = recall_score(y_test, y_pred, average='binary', pos_label=1) if len(np.unique(y))==2 else recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary', pos_label=1) if len(np.unique(y))==2 else f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    results[name] = {\n",
    "        'classifier': clf,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    print(name, 'accuracy=', acc, 'f1=', f1)\n",
    "\n",
    "# Display a confusion matrix heatmap for each model\n",
    "for name, info in results.items():\n",
    "    cm = confusion_matrix(y_test, info['y_pred'])\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion matrix: {name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "# Summary table\n",
    "summary = pd.DataFrame([{\n",
    "    'model': k,\n",
    "    'accuracy': v['accuracy'],\n",
    "    'precision': v['precision'],\n",
    "    'recall': v['recall'],\n",
    "    'f1': v['f1']\n",
    "} for k, v in results.items()])\n",
    "display(summary.sort_values('f1', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3fcd55",
   "metadata": {},
   "source": [
    "### Next\n",
    "- Use `03_Comparison_MLflow.ipynb` to run cross-validation across models and log results to MLflow for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb0861",
   "metadata": {},
   "source": [
    "## Quick Hyperparameter Tuning (light)\n",
    "The cell below runs a small RandomizedSearchCV for each model using very few iterations (n_iter=3) and `cv=2`.\n",
    "This is intended for quick experimentation inside the notebook; final tuning is done in the main flow (now also reduced for speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "tuning_results = {}\n",
    "\n",
    "# Define small search spaces for quick tuning\n",
    "param_spaces = {\n",
    "    'RF': {'n_estimators': [50,100,200], 'max_depth': [None,5,10]},\n",
    "    'KNN': {'n_neighbors': [3,5,7], 'weights': ['uniform','distance']},\n",
    "    'SVM': {'C': [0.1,1,10], 'kernel': ['rbf','linear']},\n",
    "    'NB': {},\n",
    "    'MLP': {'hidden_layer_sizes': [(50,),(100,)], 'alpha': [0.0001,0.001]}\n",
    "}\n",
    "\n",
    "for name in models:\n",
    "    print('Tuning', name)\n",
    "    params = param_spaces.get(name, {})\n",
    "    if not params:\n",
    "        print('No tuning parameters for', name)\n",
    "        continue\n",
    "    # Build estimator similar to 03_Comparison notebook\n",
    "    if name == 'RF':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        estimator = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    elif name == 'KNN':\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        estimator = KNeighborsClassifier(n_jobs=-1)\n",
    "    elif name == 'SVM':\n",
    "        from sklearn.svm import SVC\n",
    "        estimator = SVC(probability=True, random_state=42)\n",
    "    elif name == 'MLP':\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        estimator = MLPClassifier(random_state=42)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    search = RandomizedSearchCV(estimator, params, n_iter=3, cv=2, random_state=42, n_jobs=-1)\n",
    "    try:\n",
    "        search.fit(X_train, y_train)\n",
    "        print('Best params for', name, search.best_params_)\n",
    "        tuning_results[name] = search.best_params_\n",
    "    except Exception as e:\n",
    "        print('Tuning failed for', name, e)\n",
    "\n",
    "tuning_results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
